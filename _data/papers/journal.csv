paper_id,title,abstract,authors
7,CMD: Controllable Matrix Decomposition with Global Optimization for Deep Neural Network Compression,"The compression and acceleration of Deep neural networks (DNNs) are necessary steps to deploy sophisticated networks into resource-constrained hardware systems. Due to the weight matrix tends to be low-rank and sparse, several low-rank and sparse compression schemes are leveraged to reduce the overwhelmed weight parameters of DNNs. In these previous schemes, how to make the most of the low-rank and sparse components of weight matrices and how to globally decompose the weight matrix of different layers for efficient compression need to be further investigated. In this paper, in order to effectively utilize the low-rank and sparse characteristics of the weight matrix, we first introduce a sparse coefficient to dynamically control the allocation between the low-rank and sparse components, and an efficient reconstructed network is designed to reduce the inference time. Secondly, since the results of low-rank decomposition can affect the compression ratio and accuracy of DNNs, we establish an optimization problem to automatically select the optimal hyperparameters of the compressed network and achieve global compression for all the layers of network synchronously. Finally, to solve the optimization problem, we present a decomposition-searching algorithm to search the optimal solution. The algorithm can dynamically keep the balance between the compression ratio and accuracy. Extensive experiments of AlexNet, VGG-16 and ResNet-18 on CIFAR-10 and ImageNet are employed to evaluate the effectiveness of the proposed approach. After slight fine-tuning, compressed networks have gained 1.2X to 11.3X speedup and our method reduces the size of different networks by 1.4X to 14.6X.","Haonan Zhang (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University); Longjun Liu (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University)*; Hengyi Zhou (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University); Hongbin Sun (Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University); Nanning Zheng (Xi'an Jiaotong University)"
13,Towards Interpreting Deep Neural Networks via Layer Behavior Understanding,"Deep neural networks (DNNs) have achieved success in many machine learning tasks. However, how to interpret DNNs is still an open problem. In particular, how do hidden layers behave is not clearly understood. In this paper, relying on a teacher-student paradigm, we seek to understand the layer behaviors of DNNs by ``monitoring'' the distribution evolution for both across-layer and single-layer along the depth and training epochs, respectively. Relying on the optimal transport theory, we employ the Wasserstein distance (W-distance) to measure the divergence between the layer distribution and the target distribution. Theoretically, we prove that i) the W-distance between the distribution of any layer and the target distribution tends to decrease along the depth; ii) for a specific layer, the W-distance between the distribution in an iteration and the target distribution tends to decrease along training epochs; iii) a deeper layer, however, is not always better than a shallower layer. Relying on these properties, we are able to propose an early-exit inference method to improve the performance of the multi-label classification. Moreover, our results help to analyze the stability of layer distributions and explain why auxiliary losses are helpful in training DNNs. Extensive experiments justify our theoretical findings.",Jiezhang Cao (South China University of Technology); Jincheng Li (South China University of Technology); Xiping Hu (Lanzhou University); Xiangmiao Wu (SCUT); Mingkui Tan (South China University of Technology)*
15,End-to-End Entity-Aware Neural Machine Translation,"Accurate translation of entities (e.g., person names, organizations, geography) is important in neural machine translation (briefly, NMT), as they are usually more difficult to translate than other words, and an incorrect translation of them will greatly hurt user experiences. In previous works, entities are either treated in the same way as other words, which leads to inaccurate translation, or handled by multiple steps (including named entity recognition, translation, and replacing entities back), which significantly increase the inference latency. In this work, we propose an end-to-end algorithm that carefully handles the translation of entities. There are mainly two novel parts compared to conventional NMT model: (1) The encoder and the decoder are attached with entity classifiers, which are used to verify whether the input token is a named entity. In this way, the encoder and decoder are capable to treat named entities differently; (2) The translation loss of each target token is adaptively increased by the probability that the target token is a named entity, which results in more accurate translation of entities. During inference time, these two parts will be removed so that the translation model maintains the same inference speed as conventional NMT models.
Empirical results on six translation tasks demonstrate the effectiveness of our methods of improving the translation quality. Specifically, we improve 1.7 BLEU scores on Japanese to English translation and 4.6 entity F1 scores on English to Chinese translation, without additional inference cost.","Shufang Xie (Gaoling School of Artificial Intelligence, Renmin University of China)*; Yingce Xia (MSRA); Lijun Wu (Microsoft Research); Yiqing Huang (Tsinghua University); Yang Fan (University of Science and Technology of China); Tao Qin (Microsoft Research Asia)"
24,Online Strongly Convex Optimization with Unknown Delays,"We investigate the problem of online convex optimization with unknown delays, in which the feedback of a decision arrives with an arbitrary delay. Previous studies have presented delayed online gradient descent (DOGD), and achieved the regret bound of $O(\sqrt{T+D})$ by only utilizing the convexity condition, where $D$ is the sum of delays over $T$ rounds. In this paper, we further exploit the strong convexity to improve the regret bound. Specifically, we first propose a variant of DOGD for strongly convex functions, and establish a better regret bound of $O(d\log T)$, where $d$ is the maximum delay. The essential idea is to let the learning rate decay with the total number of received feedback linearly. Furthermore, we extend the strongly convex variant of DOGD and its theoretical guarantee to the more challenging bandit setting by combining with the classical $(n+1)$-point and two-point gradient estimators. To the best of our knowledge, this is the first work that solves online strongly convex optimization under the general delayed setting.",Yuanyu Wan (Nanjing University); Wei-Wei Tu (4Paradigm Inc.); Lijun Zhang (Nanjing University)*
29,On the benefits of representation regularization in invariance based domain generalization,"A crucial aspect in reliable machine learning is to design a deployable system in generalizing new related but unobserved environments. Domain generalization aims to alleviate such a prediction gap between the observed and unseen environments. Previous approaches commonly incorporated learning invariant representation for achieving good empirical performance. In this paper, we reveal that merely learning invariant representation is vulnerable to the unseen environment. To this end, we derive novel theoretical analysis to control the unseen test environment error in the representation learning, which highlights the importance of controlling the smoothness of representation. In practice, our analysis further inspires an efficient regularization method to improve the robustness in domain generalization. Our regularization is orthogonal to and can be straightforwardly adopted in existing domain generalization algorithms for invariant representation learning. Empirical results show that our algorithm outperforms the base versions in various datasets and invariance criteria.",Changjian Shui (Université Laval)*; Boyu Wang (University of Western Ontario); Christian Gagné (Université Laval)
30,Improve Generated Adversarial Imitation Learning with Reward Variance Regularization,"Imitation learning aims at recovering expert policies from limited demonstration data. Generative Adversarial Imitation Learning (GAIL) employs the generative adversarial learning framework for imitation learning and has shown great potentials. GAIL and its variants, however, are found highly sensitive to hyperparameters and hard to converge well in practice. One key issue is that the supervised learning discriminator has a much faster learning speed than the reinforcement learning generator, making the generator gradient vanishing. Although GAIL is formulated as a zero-sum adversarial game, the ultimate goal of GAIL is to learn the generator, thus the discriminator should play the role more like a teacher rather than a real opponent. Therefore, the learning of the discriminator should consider how the generator could learn. In this paper, we disclose that enhancing the gradient of the generator training is equivalent to increase the variance of the fake reward provided by the discriminator output. We thus propose an improved version of GAIL, GAIL-VR, in which the discriminator also learns to avoid generator gradient vanishing through regularization of the fake rewards variance. Experiments in various tasks, including locomotion tasks and Atari games, indicate that GAIL-VR can improve the training stability and imitation scores.","Yi-Feng Zhang (State Key Laboratory of Novel Software Technology, Nanjing University)*; Fan-Ming Luo (Nanjing University); Yang Yu (Nanjing University)"
41,Bayesian Optimization with Partially Specified Queries,"Bayesian optimization (BO) is an approach for optimizing an expensive-to-evaluate black-box function and sequentially determines values of input variables to evaluate the function.
However, it is expensive or difficult to specify values of all input variables, for example, in outsourcing scenarios where production of input queries with many input variables involves much cost.
In this paper, we propose a novel Gaussian process bandit problem, BO with partially specified queries (BOPSQ).
 In BOPSQ, unlike the standard BO setting, a learner specifies only the values of some input variables, and the values of the unspecified input variables are randomly determined according to a known or unknown distribution.
 We propose two algorithms based on posterior sampling for cases of known and unknown input distributions.
We further derive their regret bounds that are sublinear for popular kernels.
 We demonstrate the effectiveness of the proposed algorithms using test functions and real-world datasets.",Shogo Hayashi (NEC Corporation)*; Junya Honda (Kyoto University / RIKEN); Hisashi Kashima (Kyoto University)
43,A Study of BERT for Context-Aware Neural Machine Translation,"Context-aware neural machine translation (NMT), which targets at translating sentences with contextual information, has attracted much attention recently. A key problem for context-aware NMT is to effectively encode and aggregate the contextual information. BERT has been proven to be an effective feature extractor in natural language understanding tasks, but it has not been well studied in context-aware NMT. In this work, we conduct a study about leveraging BERT to encode the contextual information for NMT, and explore three commonly used methods to aggregate the contextual features. We conduct experiments on five translation tasks and find that concatenating all contextual sequences as a longer one and then encoding it by BERT obtains the best translation results. Specifically, we achieved state-of-the-art BLEU scores on several widely investigated tasks, including IWSLT'14 German-English, News Commentary v11 English-German translation and OpenSubtitle English-Russian translation.",Xueqing Wu (University of Illinois Urbana-Champaign); Yingce Xia (Microsoft Research Asia)*; Jinhua Zhu (University of Science and Technology of China); Lijun Wu (Microsoft Research); Shufang Xie (Microsoft Research Asia); Tao Qin (Microsoft Research Asia)
50,Switching: Understanding the Class-reversed Sampling in Tail Sample Memorization,"Long-tailed visual recognition poses significant challenges to traditional machine learning and emerging deep networks due to its inherent class imbalance. Existing reweighting and re-sampling methods, although effective, lack a fundamental theory while leaving the paradoxical effects of long tail unsolved, where network failing with head classes under-represented and tail classes overfitted. In this paper, we investigate long-tailed recognition from a memorization-generalization point of view, which not only unravels the whys of previous methods, but also derives a new principled solution. Specifically, we first empirically identify the regularity of classes under long-tailed distributions, finding that long-tailed challenge is essentially a trade-off between the representation of high-regularity head classes and generalization to low-regularity tail classes. To memorize tail samples without seriously damaging the representation of head samples, we propose a simple yet effective sampling strategy for ordinary mini-batch SGD optimization process, Switching, which switches from instance-balanced sampling to class-reversed sampling for only once at small learning rate. By theoretical analysis, we show that the upper bound on the generalization error of the proposed sampling strategy is lower than instance-balanced sampling conditionally. In our experiments, the proposed method can reach feasible performance more efficiently than current methods. Further experiments validate the superiority of the proposed Switching strategy, implying that the long-tailed learning trade-off could be parsimoniously tackled only in the memorization stage with a small learning rate and over-exposure of tail samples.",Chi Zhang (Xi'an Jiaotong Univiersity)*; Benyi Hu (Xi'an Jiaotong University); Yuhang Liuzhang (Xi'an Jiaotong University); Le Wang (Xi'an Jiaotong University); Li Liu (Inception Institute of Artificial Intelligence); Yuehu Liu (Xi'an Jiaotong University)
57,Improving Deep Label Noise Learning with Dual Active Label Correction,"Label noise is now a common problem in many applications, which
 may lead to significant learning performance degeneration. To deal with the label
noise, Active Label Correction (ALC) was proposed to query the true labels for
 a small subset of instances. As the true labels costs can be high, the focus of
ALC is to maximally improve the learning performance with minimal query costs.
 Existing ALC methods mainly proceed by querying the most likely mislabeled
instances, or using criteria derived from standard active learning. In this paper,
 we focus on deep neural network (dnn) models and show that due to their intrinsic
memorization effect, the true labels of a large proportion of mislabeled instances
 can be correctly predicted with early stopped training, even under severe noise.
Inspired by this, we propose to train deep label noise learning models robustly with
 dual ALC (DALC): on one hand, we select the most useful instances for classifier
improvement and query their true labels from external experts; on the other hand,
 due to the active data sampling bias, the label noise model estimation can be highly biased, which may in turn hurt the classifier learning. To alleviate this issue, we propose to identify the instances that are most likely predicted with true labels
by the classifier, and take the predictions as their true labels. By integrating the
two sources of true labels, we experiment on multiple benchmark datasets with
 various label noise rate and show the effectiveness of the proposed DALC on both
the classification accuracy and the label noise model estimation.",Shao-Yuan Li (Nanjing University of Aeronautics and Astronautics)*; Ye Shi (Nanjing University of Aeronautics and Astronautics); Sheng-Jun Huang (Nanjing University of Aeronautics and Astronautics); Songcan Chen (Nanjing University of Aeronautics and Astronautics)
58,The Flowing Nature Matters: Feature Learning from the Control Flow Graph of Source Code for Bug Localization,"Bug localization plays an important role in software maintenance. Traditional works treat the source code from the lexical perspective, while some recent researches indicate that exploiting the program structure is beneficial for improving bug localization. Control flow graph (CFG) is a widely used graph representation, which essentially represents the program structure. Although using graph neural network for feature learning is a straightforward way and has been proven effective in various software mining problems, this approach is inappropriate due to the assumption that adjacent nodes share similar semantics no longer holds in the CFG. Instead, the previous statements may affect the semantics of subsequent statements along the execution path represented by the CFG, which we call the \textit{flowing} nature of control flow graph. In this paper, we claim that the flowing nature should be explicitly considered and propose a novel model named cFlow for bug localization, which employs a particular designed flow-based GRU for feature learning from the CFG. The flow-based GRU exploits the program structure represented by the CFG to transmit the semantics of statements along the execution path, which reflects the \textit{flowing nature}. Experimental results on widely-used real-world software projects show that cFlow significantly outperforms the state-of-the-art bug localization methods, indicating that exploiting the program structure from the CFG with respect to the flowing nature is beneficial for improving bug localization.",Yi-Fan Ma (Nanjing University)*; Ming Li (Nanjing University)
64,Exploring the Common Principal Subspace of Deep Features in Neural Networks,"We find that different Deep Neural Networks (DNNs) trained with the same dataset share a common principal subspace in latent spaces, no matter in which architectures (e.g., Convolutional Neural Networks (CNNs), Multi-Layer Preceptors (MLPs) and Autoencoders (AEs)) the DNNs were built or even whether labels have been used in training (e.g., supervised, unsupervised, and self-supervised learning). Specifically, we design a new metric $\mathcal{P}$-vector to represent the principal subspace of deep features learned in a DNN, and propose to measure angles between the principal subspaces using $\mathcal{P}$-vectors. Small angles (with cosine close to $1.0$) have been found in the comparisons between any two DNNs trained with different algorithms/architectures. Furthermore, during the training procedure from random scratch, the angle decrease from a larger one ($70^\circ-80^\circ$ usually) to the small one, which coincides the progress of feature space learning from scratch to convergence. Then, we carry out case studies to measure the angle between the $\mathcal{P}$-vector and the principal subspace of training dataset, and connect such angle with generalization performance. Extensive experiments with practically-used Multi-Layer Perceptron (MLPs), AEs and CNNs for classification, image reconstruction, and self-supervised learning tasks on MNIST, CIFAR-10 and CIFAR-100 datasets have been done to support our claims with solid evidences.",Haoran Liu (Texas A&M University); Haoyi Xiong (Baidu Research)*; Yaqing Wang (Baidu Research); Haozhe An (Baidu Research); Dongrui Wu (Huazhong University of Science and Technology); Dejing Dou (Baidu)
66,Worst-case Regret Analysis of Computationally Budgeted Online Kernel Selection,"We study the problem of online kernel selection under computational constraints, where the memory or time of kernel selection and online prediction procedures is restricted to a fixed budget. In this paper, we analyze the worst-case lower bounds on the regret attainable by any online kernel selection algorithm that operates on a subset of the observed examples, and develop algorithms achieving matching upper bounds. We also identify the condition under which online kernel selection with time constraints is different from that with memory constraints. To design algorithms, we reduce the problems to two sequential decision problems, that is, a problem of prediction with expert advice and a multi-armed bandit problem with an additional observation. Our algorithms contain some new techniques, such as memory sharing, hypothesis space discretization and decoupled exploration-exploitation scheme. Numerical experiments are conducted to verify our theoretical results.",Junfan Li (Tianjin University); Shizhong Liao (Tianjin University)*
68,Multiple Partitions Alignment via Spectral Rotation,"Multi-view spectral clustering has drawn much attention due to the effectiveness of exploiting the similarity relationships among data points. These methods typically reveal the intrinsic structure using a predefined graph for each view. The predefined graphs are fused to a consensus one, on which the final clustering results are obtained. However, such common strategies may lead to information loss because of the inconsistency or noise among multiple views. In this paper, we propose to merge multi-view information in partition level instead of the raw feature space where the data points lie. The partition of each view is treated as a perturbation of the consensus clustering, and the multiple partitions are integrated by estimating a distinct rotation for each partition. The proposed model is formulated as a joint learning framework, i.e., with the input data matrix, our model directly outputs the final discrete clustering result. Hence it is an end-to-end single-stage learning model. An iterative updating algorithm is proposed to solve the learning problem, in which the involved variables can be optimized in a mutual reinforcement manner. Experimental results on real-world data sets illustrate the effectiveness of our model.",Shudong Huang (Sichuan University)*; Ivor Tsang (University of Technology Sydney); Zenglin Xu (Harbin Institute of Technology); Jiancheng Lv (Sichuan University)
72,Improving Kernel Online Learning with a Snapshot Memory,"We propose in this paper the Stochastic Variance-reduced Gradient Descent for Kernel Online Learning (DualSVRG), which obtains the \varepsilon-approximate linear convergence rate and is not vulnerable to the curse of kernelization. Our approach uses the variance reduction technique to reduce the variance when estimating full gradient, and further exploits recent work in dual space gradient descent for online learning to achieve model optimality. This is achieved by introducing the concept of an instant memory, which is a snapshot storing the most recent incoming data instances and proposing three transformer oracles, namely budget, coverage, and always-move oracles. We further develop rigorous theoretical analysis to demonstrate that our proposed approach can obtain the \varepsilon-approximate linear convergence rate, while maintaining model sparsity, hence encourages fast training. We conduct extensive experiments on several benchmark datasets to compare our DualSVRG with state-of-the-art baselines in both batch and online settings. The experimental results show that our DualSVRG yields superior predictive performance, while spending comparable training time with baselines.",Trung Le (Monash University)*; Khanh Nguyen (HCMUP); Dinh Phung (Monash University)