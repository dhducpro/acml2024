- title: "Battle of Bandits: Online learning from Preference Feedback"
  start_at: '2021-11-17 09:00:00 +08:00'
  end_at: '2021-11-17 11:30:00 +08:00'
  abstract: "This tutorial would cover the development and recent progress on Preference based Bandits where   the goal is to sequentially learn the best-action of a decision set from preference feedback over an actively   chosen subset of items. We initially start with a brief overview of the motivation and problem formulation,   and then understand the breakthrough results for the simplest pairwise preference setting (where the subsets   are of size 2), famously studied as the ‘Dueling Bandit’ problem in the literature. Will then generalize it to the   ‘Battling Bandits’ framework for subsets of any arbitrary size and understand the tradeoff between learning   rates-vs-increasing subset sizes."
  biographies:
    - name: Aadirupa Saha
      bio: "Aadirupa Saha is a postdoctoral researcher at Microsoft Research New York City. Earlier she was a PhD
      student at Indian Institute of Science, Bangalore. Her research interests broadly lie in the areas of Bandits
      and Reinforcement learning, Optimization, Learning theory, and Algorithm analysis. She currently works on
      developing large-scale robust algorithms for various sequential online decision-making problems under preference
      information or more generally partial monitoring feedback."

- title: "Optimal Transport"
  start_at: '2021-11-17 09:00:00 +08:00'
  end_at: '2021-11-17 11:30:00 +08:00'
  abstract: "Optimal transport has a long history in mathematics which was proposed by Gaspard Monge in the eighteenth century [Old/New book]. The theory was later investigated by Nobelists (for a joint prize in economic sciences) Koopmans and Kantorovich as well as Fields medalists Villani (2010) and Figalli (2018). Recently, advances in optimal transport theory have paved the way for its use in the ML/AI community, particularly for formulating models and learning with high-dimensional data. This tutorial aims to introduce pivotal computational, practical aspects of OT as well as applica- tions of OT for unsupervised learning problems. The topics of this tutorial consist of three main parts. In the first part, we will present the theoretical and computational background of optimal transport theory. In the second part, we will summarize the application of OT estimating of deep generative models. The clustering and topic modelling methods with OT will be summarized in the last part of the tutorial. Implementation of algorithms and illustrative examples will also be presented."
  biographies:
    - name: Viet Huynh
      bio: "Viet Huynh is currently a postdoctoral researcher at the Machine Learning team at Monash University. Before going to Monash, he was a postdoctoral researcher in the PRaDA (Pattern Recognition and Data Analytics) center at Deakin University. He was a PhD student in PRaDA center at Deakin University from 2013 to early 2017. He worked under the supervision of Professor Dinh Phung and Professor Svetha Venkatesh. His PhD work focused on resorting big data to actionable information involves dealing with four dimensions of challenges in big data (called four Vs): volume, variety, velocity, veracity. He also received his B.Eng. and M.Eng. degrees in computer science in 2005 and 2009 respectively, all of which were completed at the University of Technology, Vietnam. He is interested in developing large-scale learning algorithms for probabilistic graphical models with complex and large-scale data, applying optimal transport theory to understand challenging problems in machine learning and deep learning, and applying deep generative models for learning with probabilistic graphical models. His research has been published in prestigious machine learning venues such as the Journal of Machine Learning Research (JMLR), JMLR, NeurIPS, ICML, ICLR, AISTATS, etc."
    - name: He Zhao
      bio: "He Zhao obtained his PhD degree in machine learning at the Department of Data Sci- ence and AI (DSAI) of Monash University in 2019 and has been working as a research fellow at the department since then, under the supervision of Prof Dinh Phung. Be- fore coming to Monash, he got his bachelor’s and master’s degrees from Nankai Uni- versity and Nanjing University, respectively. He is interested in statistical machine learning and deep learning, including Bayesian statistics, optimal transport, robust machine learning, as well as their applications in computer vision, natural language processing, data mining, etc. His research has been published prestigious machine learning venues such as NeurIPS, ICML, and ICLR. He is a senior program committee (PC) member of IJCAI 2021 and regularly serves as a reviewer or PC member for leading computer science conferences and journals including NeurIPS, ICML, ICLR, TPAMI, JMLR, etc."
    - name: Nhat Ho
      bio: "Nhat Ho is currently an Assistant Professor of Statistics and Data Sciences at the University of Texas at Austin. He is also a core member of the Machine Learning Laboratory. Before going to Austin, he was a postdoctoral fellow in the Electrical Engineering and Computer Science (EECS) Department under the mentorship of Professor Michael I. Jordan and Professor Martin J. Wainwright. Going further back in time, he finished his Phd degree in 2017 at the Department of Statistics, University of Michigan, Ann Arbor where his advisors are Professor Long Nguyen and Professor Ya’acov Ritov. A central theme of his research focuses on four principles of machine learning, statistics, and data science: heterogeneity of data, interpretability of models, stability and scalability of optimization and sampling algorithms. He has published and submitted over 50 papers in the top conferences and journals of machine learning, statistics, and data science, such as ICML, NeurIPS, ICLR, AISTATS, ICCV, Journal of Machine Learning Research (JMLR), Annals of Statistics, and SIAM Journal on Mathematics of Data Science."
    - name: Dinh Phung
      bio: "Dinh Phung is currently a Professor in the Faculty of Information Technology, Monash University, Australia. He obtained a PhD from Curtin University in 2005 in the areas of machine learning and multimedia computing. His primary interest includes theoretical and applied machine learning with a current focus on deep learning, robust and adversarial ML, optimal transport and point process theory for ML, generative AI, Bayesian nonparametrics and graphical models. He publishes regularly, with over 250+ publications, in the areas of machine learning, AI, data science and application domains such as natural language processing (NLP), computer vision, digital health, cybersecurity and autism. He has delivered several invited and keynote talks, served on 40+ organizing committees and technical program committees for top-notch conferences in machine learning and data analytics. He is currently the lead Editor-in-Chief for the forthcoming 3rd edition of the Encyclopedia of Machine Learning and Data Science and was also the Finalist for Australian Museum Eureka Prize for Excellence in Data Science in 2020."

- title: "Automated Learning form Graph-Structured Data"
  start_at: '2021-11-17 12:00:00 +08:00'
  end_at: '2021-11-17 14:30:00 +08:00'
  abstract: "Graph-structured data (GSD) is ubiquitous in real-life applications, which appears in many learning applications such as property prediction for molecular graphs, product recommendations from heterogeneous information networks, and logical queries from knowledge graphs. Recently, learning from graph-structured data has also become a research focus in the machine learning community. However, again due to such diversities in GSD, there are no universal learning models that can perform well and consistently across different learning applications based on graphs. In sharp contrast to this, convolutional neural networks work well on natural images, and transformers are good choices for text data. In this tutorial, we will talk about using automated machine learning (AutoML) as a tool to design learning models for GSD. Specifically, we will elaborate on what is AutoML, what kind of prior information from graphs can be explored by AutoML, and how can insights be generated from the searched models."
  biographies:
    - name: Dr. Quanming Yao
      bio: "Dr. Quanming Yao is a tenure-track assistant professor in the Department of Electronic Engineering, Tsinghua University. He was a senior scientist in 4Paradigm Inc., who is also the founding leader of the company’s machine learning research team. He obtained his Ph.D. degree at the Department of Computer Science and Engineering of Hong Kong University of Science and Technology (HKUST). His research interests are in machine learning, optimization, and automated machine learning. He has 40+ top-tier journal and conference papers, including ICML, NeurIPS, JMLR, and TPAMI, with a citation of 2300 (since 2015). He is a receipt of Forbes 30 Under 30 (China), Young Scientist Awards (issued by Hong Kong Institution of Science), Wuwen Jun Prize for Excellence Youth of Artificial Intelligence (issued by CAAI), and a winner of Google Fellowship (in machine learning)."
    - name: Dr. Huan Zhao
      bio: "Dr. Huan Zhao is a senior scientist in 4Paradigm Inc., China, leading the research on automated graph representation learning (AutoGraph) in the company. Prior to 4Paradigm, He worked as a research intern and senior algorithm engineer in Alibaba from November 2017 to July 2019. He obtained his Doctor Degree at the Department of Computer Science and Engineering, HKUST in Jan. 2019. His research interest includes recommender system, graph representation learning, and automated machine learning. He has published more than 20 top-tier conference and journal papers, including KDD, CIKM, AAAI, TKDE, TKDD, with a citation of 782 in Google Scholar (since 2016). Besides research, he also lead a team in 4Paradigm to deliver AutoGraph algorithms to real-world applications, retailing recommendation, financial fraud detection, bioinformatics, etc., from the business partners of 4Paradigm."
    - name: Dr. Yongqi Zhang
      bio: "Dr. Yongqi Zhang is a research scientist in 4Paradigm. He obtained his Ph.D. degree at the Department of Computer Science and Engineering of Hong Kong University of Science and Technology (HKUST) in 2020 and received his bachelor degree at Shanghai Jiao Tong University (SJTU) in 2015. He has published four top-tier conference/journal papers as first-author in NeurIPS, ICDE, VLDB-Journal. His research interests focus on knowledge graph embedding, automated machine learning and deep learning. He was a Program Committee for AAAI 2020-2021, IJCAI 2020-2021, CIKM 2021, and a reviewer for TKDE and NETNET journals."

- title: "Learning under Noisy Supervision"
  website: "https://wsl-workshop.github.io/acml21-tutorial"
  start_at: '2021-11-17 15:00:00 +08:00'
  end_at: '2021-11-17 17:30:00 +08:00'
  abstract: "Noisy data is ubiquitous and harms the performance of most learning algorithms, and sometimes makes existing algorithms break down. This tutorial summarizes the most recent noisy-supervision-tolerant techniques, from the viewpoint of statistical learning, deep learning and their applications in industry."
  biographies: 
    - name: Masashi Sugiyama
      bio: "Masashi Sugiyama is Director of RIKEN Center for Advanced Intelligence Project and Professor at the University of Tokyo. He received the PhD degree in computer science from Tokyo Institute of Technology. His research is designing statistical data analysis algorithms for challenging problems. He (co)-authored machine learning monographs such as Machine Learning in Non-Stationary Environments (MIT Press), Density Ratio Estimation in Machine Learning (Cambridge University Press), Statistical Reinforcement Learning (Chapman and Hall/CRC), Introduction to Statistical Machine Learning (Morgan Kaufmann), and Variational Bayesian Learning Theory (Cambridge University Press). He served as Program Co-chair for the Neural Information Processing Conference, International Conference on Artificial Intelligence and Statistics, and Asian Conference on Machine Learning. He serves as an Associate Editor for the IEEE Transactions on Pattern Analysis and Machine Intelligence and an Action Editor for the Neural Network journal. He received the Japan Academy Medal in 2017."
    - name: Tongliang Liu
      bio: "Tongliang Liu is a Lecturer in Machine Learning with the University of Sydney (USYD). He organized a tutorial on learning with label noise at a main Australian computer vision conference (DICTA) in 2017. Web: http://dicta2017.dictaconference.org/tutorial.html. He has published around 20 papers on learning with Noisy Supervision at top venues."
    - name: Bo Han
      bio: "Bo Han is an Assistant Professor of Computer Science at Hong Kong Baptist University, and a Visiting Scientist at RIKEN Center for Advanced Intelligence Project (RIKEN AIP). He was a Postdoc Fellow at RIKEN AIP (2019-2020). He received his Ph.D. degree in Computer Science from University of Technology Sydney in 2019. He has served as area chairs of NeurIPS and ICLR. He received the RIKEN BAIHO Award (2019) and RGC Early Career Scheme (2020)."
    - name: Quanming Yao
    - name: Gang Niu
      bio: "Gang Niu is currently a research scientist (indefinite-term) at RIKEN Center for Advanced Intelligence Project. He received the PhD degree in computer science from Tokyo Institute of Technology in 2013. Before joining RIKEN as a research scientist, he was a senior software engineer at Baidu and then an assistant professor at the University of Tokyo. He has published more than 70 journal articles and conference papers, including 14 NeurIPS (1 oral and 3 spotlights), 28 ICML, and 2 ICLR (1 oral) papers. He has served as an area chair 14 times, including ICML 2019–2021, NeurIPS 2019–2021, and ICLR 2021–2022."
      
- title: "Differential geometry for generative modeling"
  start_at: '2021-11-17 18:00:00 +08:00'
  end_at: '2021-11-17 20:30:00 +08:00'
  abstract: "Differential geometry is playing an increasing role in manifold learning, and generative modeling in particular. Geometry provides us with well-defined and well-behaved tools for interpolation and statistical analysis on the learned manifolds, and provides a principled solution to the identifiability problem that plagues many generative models. While the geometric approach is elegant it comes with a steep learning curve as the literature is developed from a mathematical rather than applied perspective. In this tutorial we first develop the classic differential geometry needed to understand deterministic manifolds, and then show how this applies to the stochastic setting. We show how to turn the mathematical concepts into simple algorithms that allow for principled data analysis over learned manifolds. Importantly, we require little more mathematical background from the audience than knowledge of Taylor expansions."
  biographies:
    - name: Søren Hauberg
      bio: "I am a professor of Geometry in Machine Learning at the Technical University of Denmark. I received his PhD in computer science from the University of Copenhagen in 2011. During my PhD I spend 6 months as a visiting scholar at UC Berkeley working with Ruzena Bajcsy. Prior to pursuing a PhD I worked as a ”digital lumberjack” in the startup Dralle A/S. I was a postdoc for two years at Perceiving Systems at the Max Planck Institute for Intelligent Systems working with Michael Black. In 2013, I was the sole computer science recipient of the Sapere Aude Research Talent award from the Danish Council for Independent Research, and in 2016 I was the sole computer science Villum Young Investigator. In 2017 I was further awarded a starting grant from the European Research Council. In 2018, I joined the Young Scientists community under the World Economic Forum, and was in the process named one of ‘10 of the most exciting young scientists working in the world today.’ My research interest lie in the span of geometry and statistics. I develop machine learning techniques using geometric constructions, and work on the related numerical challenges. I am particularly interested in random geometries as they naturally appear in learning."
